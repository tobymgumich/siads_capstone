{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7485dc87-656f-408f-9fa2-c8e1f9911880",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.40.6-py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from openai) (0.27.0)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Using cached jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from openai) (1.10.16)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/conda/lib/python3.10/site-packages (from openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/conda/lib/python3.10/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Downloading openai-1.40.6-py3-none-any.whl (361 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m361.3/361.3 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
      "Installing collected packages: jiter, openai\n",
      "Successfully installed jiter-0.5.0 openai-1.40.6\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e9277d5-466a-4e76-946d-e84ac2209972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: pydantic 1.10.16\n",
      "Uninstalling pydantic-1.10.16:\n",
      "  Successfully uninstalled pydantic-1.10.16\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall pydantic -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cf1f8c1-34b9-4e8b-b935-c9d11d0747ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydantic==1.10.8\n",
      "  Using cached pydantic-1.10.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (146 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic==1.10.8) (4.12.2)\n",
      "Using cached pydantic-1.10.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Installing collected packages: pydantic\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.7.3\n",
      "    Uninstalling pydantic-2.7.3:\n",
      "      Successfully uninstalled pydantic-2.7.3\n",
      "Successfully installed pydantic-1.10.8\n"
     ]
    }
   ],
   "source": [
    "!pip install pydantic==1.10.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83b04629-372c-4d00-9cb7-311d236e37bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77045f58-032f-4fbc-bbe5-a36f534234a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "520b7ed9-bf84-4ca3-83cb-6ce152808b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc773a6c-b9fc-4556-a9a4-c034dff8fdf5",
   "metadata": {},
   "source": [
    "# Sample text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "210a4727-5161-4272-ba90-20bf04422b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_content_path = '../wiki_content.csv'\n",
    "wiki_content_df = pd.read_csv(wiki_content_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "088e8acf-9ca5-4611-aeb8-44a6f24a5f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "section_test = wiki_content_df.sample()\n",
    "text_test = section_test['content'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2478646e-bbd8-4d42-bd3b-60d1bc459b7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Spatial inference, or estimation, of a quantity Z : : R n → → R {\\\\displaystyle Z\\\\colon \\\\mathbb {R} ^{n}\\\\to \\\\mathbb {R} } , at an unobserved location x 0 {\\\\displaystyle x_{0}} , is calculated from a linear combination of the observed values z i = Z ( x i ) {\\\\displaystyle z_{i}=Z(x_{i})} and weights w i ( x 0 ) , i = 1 , … … , N {\\\\displaystyle w_{i}(x_{0}),\\\\;i=1,\\\\ldots ,N} : The weights w i {\\\\displaystyle w_{i}} are intended to summarize two extremely important procedures in a spatial inference process: reflect the structural \"proximity\" of samples to the estimation location x 0 {\\\\displaystyle x_{0}} ; at the same time, they should have a desegregation effect, in order to avoid bias caused by eventual sample clusters. When calculating the weights w i {\\\\displaystyle w_{i}} , there are two objectives in the geostatistical formalism: unbias and minimal variance of estimation. If the cloud of real values Z ( x 0 ) {\\\\displaystyle Z(x_{0})} is plotted against the estimated values Z ^ ^ ( x 0 ) {\\\\displaystyle {\\\\hat {Z}}(x_{0})} , the criterion for global unbias, intrinsic stationarity or [wide sense stationarity](https://en.wikipedia.org/wiki/Stationary_process) of the field, implies that the mean of the estimations must be equal to mean of the real values. The second criterion says that the mean of the squared deviations ( Z ^ ^ ( x ) − − Z ( x ) ) {\\\\displaystyle {\\\\big (}{\\\\hat {Z}}(x)-Z(x){\\\\big )}} must be minimal, which means that when the cloud of estimated values versus the cloud real values is more disperse, the estimator is more imprecise.'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23362143-3817-4dee-885e-169603be9157",
   "metadata": {},
   "source": [
    "## Paraphrasing Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca129b2b-4914-4d18-b5ba-7e917d8dfd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_para_examples = [\n",
    "        {\n",
    "            \"text\": \"Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics and other fields, to find a linear combination of features that characterizes or separates two or more classes of objects or events.\",\n",
    "            \"answer\": [\"A statistical technique known as linear discriminant analysis (LDA), also referred to as normal discriminant analysis (NDA) or discriminant function analysis, expands upon Fisher's linear discriminant method. This approach is employed in various disciplines to identify a linear combination of attributes that distinguishes or defines multiple categories of objects or occurrences.\"]\n",
    "        },\n",
    "        {\n",
    "            \"text\": \"Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics and other fields, to find a linear combination of features that characterizes or separates two or more classes of objects or events.\",\n",
    "            \"answer\": [\"Originating from Fisher's linear discriminant, linear discriminant analysis (LDA) - also called normal discriminant analysis (NDA) or discriminant function analysis - is a statistical methodology utilized across different fields. Its primary purpose is to determine a linear blend of characteristics that effectively differentiates or describes two or more groups of entities or phenomena.\"]\n",
    "        },\n",
    "        {\n",
    "            \"text\": \"Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics and other fields, to find a linear combination of features that characterizes or separates two or more classes of objects or events.\",\n",
    "            \"answer\": [\"As an extension of Fisher's linear discriminant, linear discriminant analysis (LDA), which is also known as normal discriminant analysis (NDA) or discriminant function analysis, is a statistical approach employed in numerous domains. This method aims to discover a linear amalgamation of features capable of characterizing or segregating multiple classes of items or events.\"]\n",
    "        }\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6261e2-2ec5-4c30-a4eb-df9d95fe11af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plagiariser_agent_para(text_test):\n",
    "    \"\"\"Paraphrasing: Restating the original text in different words while maintaining the same meaning and level of detail.\"\"\"\n",
    "    few_shot_prompt = \"Here are some examples of how to paraphrase:\\n\\n\"\n",
    "    for example in few_shot_para_examples:\n",
    "        few_shot_prompt += f\"Text: {example['text']}\\n\"\n",
    "        few_shot_prompt += f\"Answer: {example['answer']}\\n\\n\"\n",
    "\n",
    "    full_prompt = (\n",
    "        f\"{few_shot_prompt}\"\n",
    "        f\"Now, paraphrase the following text:\\n\"\n",
    "        f\"{text_test}\\n\"\n",
    "        \"Return the results as a Python list.\"\n",
    "    )\n",
    "    \n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You paraphrase text and respond in Python lists.\"},\n",
    "            {\"role\": \"user\", \"content\": full_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    answer = completion.choices[0].message.content\n",
    "    \n",
    "    try:\n",
    "        parsed_answer = eval(answer)\n",
    "        if isinstance(parsed_answer, list):\n",
    "            return parsed_answer\n",
    "        else:\n",
    "            return [answer]\n",
    "    except:\n",
    "        return [answer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff7672fb-a4a1-4292-aa18-3014a3cada3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Spatial inference involves estimating a quantity Z at an unobserved location x0 by using a linear combination of observed values Z(xi) and weights wi(x0). These weights play a crucial role in summarizing two key aspects in spatial inference: reflecting sample proximity to the estimation location x0 and preventing bias due to sample clustering. In geostatistical formalism, the objectives for calculating weights are to ensure unbiased estimates and minimize estimation variance. Meeting the criteria of global unbias and field stationarity involves ensuring the mean of estimations matches the mean of real values, and minimizing the mean of squared deviations between estimated and real values indicates precision.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plagiariser_agent_para(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a9bc1b39-edfd-411a-bead-fa63b74e8e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_paraphrases(text_test):\n",
    "    \"\"\"\n",
    "    Generate multiple paraphrases of the input text and analyze common unigrams.\n",
    "\n",
    "    Args:\n",
    "    text_test (str): The text to be paraphrased and analyzed.\n",
    "\n",
    "    Returns:\n",
    "    list: Common unigrams present in all generated paraphrases.\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    all_unigrams = []\n",
    "    \n",
    "    for _ in range(3):\n",
    "        paraphrase = plagiariser_agent_para(text_test)\n",
    "        paraphrase = ' '.join(paraphrase) if isinstance(paraphrase, list) else paraphrase\n",
    "        tokens = word_tokenize(paraphrase.lower())\n",
    "        unigrams = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "        all_unigrams.append(unigrams)\n",
    "    \n",
    "    common_unigrams = set(all_unigrams[0])\n",
    "    for unigram_list in all_unigrams[1:]:\n",
    "        common_unigrams.intersection_update(unigram_list)\n",
    "    \n",
    "    return list(common_unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c48f46e0-92bb-4609-9e75-1b00deb1a1f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['linear',\n",
       " 'spatial',\n",
       " 'geostatistical',\n",
       " 'sample',\n",
       " 'unbiased',\n",
       " 'variance',\n",
       " 'include',\n",
       " 'bias',\n",
       " 'quantity',\n",
       " 'combination',\n",
       " 'global',\n",
       " 'objectives',\n",
       " 'location',\n",
       " 'proximity',\n",
       " 'z',\n",
       " 'weights',\n",
       " 'involves',\n",
       " 'real',\n",
       " 'values',\n",
       " 'unobserved',\n",
       " 'estimating',\n",
       " 'inference',\n",
       " 'observed']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_paraphrases(text_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8bd857-1e7c-41cf-b25f-392874a1c729",
   "metadata": {},
   "source": [
    "## Rewriting Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "719ce083-9d5c-45cc-93c7-b1ad34a7e17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_rewrite_examples = [\n",
    "        {#Rewrite (Simplified and Expanded):\n",
    "            \"text\": \"Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics and other fields, to find a linear combination of features that characterizes or separates two or more classes of objects or events.\",\n",
    "            \"answer\": [\"Fisher's linear discriminant laid the groundwork for a powerful statistical technique known as Linear Discriminant Analysis (LDA). This method, also referred to as Normal Discriminant Analysis (NDA) or Discriminant Function Analysis, has found applications beyond statistics, reaching into various scientific and analytical fields. At its core, LDA seeks to identify a unique blend of characteristics that can effectively distinguish between multiple groups of items or occurrences. By doing so, it provides a valuable tool for classification and data interpretation across diverse disciplines.\"]\n",
    "        },\n",
    "        {#Rewrite (Restructured and Technical Focus):\n",
    "            \"text\": \"Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics and other fields, to find a linear combination of features that characterizes or separates two or more classes of objects or events.\",\n",
    "            \"answer\": [\"In the realm of multivariate statistics, Linear Discriminant Analysis (LDA) stands out as a versatile classification algorithm. Evolving from Fisher's original linear discriminant, LDA—also known as Normal Discriminant Analysis (NDA) or Discriminant Function Analysis—operates by projecting data onto a lower-dimensional space. Its primary objective is to maximize the separation between different classes while minimizing the spread within each class. This is achieved by identifying an optimal linear combination of features, making LDA an invaluable tool in pattern recognition, machine learning, and data mining across various scientific domains.\"]\n",
    "        },\n",
    "        {#Rewrite (Practical Application Emphasis):\n",
    "            \"text\": \"Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics and other fields, to find a linear combination of features that characterizes or separates two or more classes of objects or events.\",\n",
    "            \"answer\": [\"Imagine you're tasked with sorting a mixed basket of fruits into distinct categories based on their characteristics. This is essentially what Linear Discriminant Analysis (LDA) does, but with complex datasets. As an extension of Fisher's linear discriminant, LDA—sometimes called Normal Discriminant Analysis (NDA) or Discriminant Function Analysis—is a statistical method that has found its way into numerous fields beyond pure mathematics. It works by finding the best way to combine different features of your data, creating a sort of 'super characteristic' that can tell different groups apart. Whether you're in biology studying gene expression, in finance predicting market trends, or in image processing distinguishing objects, LDA provides a powerful means of classification and analysis.\"]\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "69c18016-ce33-431d-8b47-7d37fd12a1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plagiariser_agent_rewrite(text_test):\n",
    "    \"\"\"Rewriting: Altering the original text more significantly, which may involve restructuring, simplifying, or expanding on the content.\"\"\"\n",
    "    few_shot_prompt = \"Here are some examples of how to rewrite:\\n\\n\"\n",
    "    for example in few_shot_rewrite_examples:\n",
    "        few_shot_prompt += f\"Text: {example['text']}\\n\"\n",
    "        few_shot_prompt += f\"Answer: {example['answer']}\\n\\n\"\n",
    "\n",
    "    full_prompt = (\n",
    "        f\"{few_shot_prompt}\"\n",
    "        f\"Now, rewrite the following text:\\n\"\n",
    "        f\"{text_test}\\n\"\n",
    "        \"Return the results as a Python list.\"\n",
    "    )\n",
    "    \n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You rewrite text and respond in Python lists.\"},\n",
    "            {\"role\": \"user\", \"content\": full_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    answer = completion.choices[0].message.content\n",
    "    \n",
    "    try:\n",
    "        parsed_answer = eval(answer)\n",
    "        if isinstance(parsed_answer, list):\n",
    "            return parsed_answer\n",
    "        else:\n",
    "            return [answer]\n",
    "    except:\n",
    "        return [answer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bb23f1-2ec6-45e1-916a-696db4d1297c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_rewrites(text_test):\n",
    "    \"\"\"\n",
    "    Generate multiple rewrites of the input text and analyze common unigrams.\n",
    "\n",
    "    Args:\n",
    "    text_test (str): The text to be rewritten and analyzed.\n",
    "\n",
    "    Returns:\n",
    "    list: Common unigrams present in all generated rewrites.\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    all_unigrams = []\n",
    "    \n",
    "    for _ in range(3):\n",
    "        rewrite = plagiariser_agent_rewrite(text_test)\n",
    "        rewrite = ' '.join(rewrite) if isinstance(rewrite, list) else rewrite\n",
    "        tokens = word_tokenize(rewrite.lower())\n",
    "        unigrams = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "        all_unigrams.append(unigrams)\n",
    "    \n",
    "    common_unigrams = set(all_unigrams[0])\n",
    "    for unigram_list in all_unigrams[1:]:\n",
    "        common_unigrams.intersection_update(unigram_list)\n",
    "    \n",
    "    return list(common_unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6ec3ac82-6871-46c8-bd72-f17b72e48c8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['estimation',\n",
       " 'spatial',\n",
       " 'also',\n",
       " 'deviations',\n",
       " 'geostatistical',\n",
       " 'ensuring',\n",
       " 'unbiased',\n",
       " 'variance',\n",
       " 'quantity',\n",
       " 'global',\n",
       " 'realm',\n",
       " 'samples',\n",
       " 'process',\n",
       " 'location',\n",
       " 'proximity',\n",
       " 'z',\n",
       " 'weights',\n",
       " 'involves',\n",
       " 'real',\n",
       " 'values',\n",
       " 'estimator',\n",
       " 'minimal',\n",
       " 'observed']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_rewrites(text_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bcb36f-c6f1-4b37-89a2-ec81f91a8ebe",
   "metadata": {},
   "source": [
    "## Summarising Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9dd02f79-6071-41be-bcd9-42a40fdf56e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_summary_examples = [\n",
    "        {#Concise Summary:\n",
    "            \"text\": \"Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics and other fields, to find a linear combination of features that characterizes or separates two or more classes of objects or events.\",\n",
    "            \"answer\": [\"LDA, also known as NDA or discriminant function analysis, is a statistical method that extends Fisher's linear discriminant to classify or distinguish between multiple groups based on their features.\"]\n",
    "        },\n",
    "        {#Technical Summary:\n",
    "            \"text\": \"Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics and other fields, to find a linear combination of features that characterizes or separates two or more classes of objects or events.\",\n",
    "            \"answer\": [\"Linear Discriminant Analysis (LDA) generalizes Fisher's linear discriminant technique. It's a statistical method used across various fields to identify linear feature combinations that optimally characterize or separate multiple classes of objects or events.\"]\n",
    "        },\n",
    "        {#Contextual Summary:\n",
    "            \"text\": \"Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics and other fields, to find a linear combination of features that characterizes or separates two or more classes of objects or events.\",\n",
    "            \"answer\": [\"LDA, NDA, or discriminant function analysis builds upon Fisher's linear discriminant. This statistical approach, applicable in numerous disciplines, aims to find the best linear combination of features for differentiating between two or more categories of items or occurrences.\"]\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da1201e-204c-48bc-8f64-78449a91edd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plagiariser_agent_summary(text_test):\n",
    "    \"\"\"Summarisation: captures the essence of the original text while presenting the information in a more condensed form.\"\"\"\n",
    "    few_shot_prompt = \"Here are some examples of how to summarise:\\n\\n\"\n",
    "    for example in few_shot_summary_examples:\n",
    "        few_shot_prompt += f\"Text: {example['text']}\\n\"\n",
    "        few_shot_prompt += f\"Answer: {example['answer']}\\n\\n\"\n",
    "\n",
    "    full_prompt = (\n",
    "        f\"{few_shot_prompt}\"\n",
    "        f\"Now, summarise the following text:\\n\"\n",
    "        f\"{text_test}\\n\"\n",
    "        \"Return the results as a Python list.\"\n",
    "    )\n",
    "    \n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You summarise text and respond in Python lists.\"},\n",
    "            {\"role\": \"user\", \"content\": full_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    answer = completion.choices[0].message.content\n",
    "    \n",
    "    try:\n",
    "        parsed_answer = eval(answer)\n",
    "        if isinstance(parsed_answer, list):\n",
    "            return parsed_answer\n",
    "        else:\n",
    "            return [answer]\n",
    "    except:\n",
    "        return [answer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6860fef9-eea9-4d03-a2cb-e20bfdd066b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_summaries(text_test):\n",
    "    \"\"\"\n",
    "    Generate multiple summaries of the input text and analyze common unigrams.\n",
    "\n",
    "    Args:\n",
    "    text_test (str): The text to be summarized and analyzed.\n",
    "\n",
    "    Returns:\n",
    "    list: Common unigrams present in all generated summaries.\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    all_unigrams = []\n",
    "    \n",
    "    for _ in range(3):\n",
    "        summary = plagiariser_agent_summary(text_test)\n",
    "        summary = ' '.join(summary) if isinstance(summary, list) else summary\n",
    "        tokens = word_tokenize(summary.lower())\n",
    "        unigrams = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "        all_unigrams.append(unigrams)\n",
    "    \n",
    "    common_unigrams = set(all_unigrams[0])\n",
    "    for unigram_list in all_unigrams[1:]:\n",
    "        common_unigrams.intersection_update(unigram_list)\n",
    "    \n",
    "    return list(common_unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a7459e34-7c96-4685-b2c8-d90739859146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['linear',\n",
       " 'estimation',\n",
       " 'spatial',\n",
       " 'deviations',\n",
       " 'unbiased',\n",
       " 'quantity',\n",
       " 'combination',\n",
       " 'global',\n",
       " 'using',\n",
       " 'unbias',\n",
       " 'equal',\n",
       " 'squared',\n",
       " 'location',\n",
       " 'proximity',\n",
       " 'z',\n",
       " 'weights',\n",
       " 'involves',\n",
       " 'real',\n",
       " 'estimations',\n",
       " 'values',\n",
       " 'unobserved',\n",
       " 'estimating',\n",
       " 'x0',\n",
       " 'inference',\n",
       " 'mean',\n",
       " 'minimal',\n",
       " 'observed']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_summaries(text_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52633e67-c1d6-488b-988e-df2383c66855",
   "metadata": {},
   "source": [
    "## Keyword Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c68191d0-aaf9-4e37-9eb3-583ad3fd3751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keywords():\n",
    "    \"\"\"\n",
    "    Extract common keywords across paraphrases, rewrites, and summaries of the global text_test.\n",
    "\n",
    "    Returns:\n",
    "    list: Keywords common to all three text manipulation methods.\n",
    "    \"\"\"\n",
    "    paraphrase_keywords = analyze_paraphrases(text_test)\n",
    "    rewrite_keywords = analyze_rewrites(text_test)\n",
    "    summary_keywords = analyze_summaries(text_test)\n",
    "    \n",
    "    paraphrase_set = set(paraphrase_keywords)\n",
    "    rewrite_set = set(rewrite_keywords)\n",
    "    summary_set = set(summary_keywords)\n",
    "    \n",
    "    final_keywords = list(paraphrase_set.intersection(rewrite_set, summary_set))\n",
    "    \n",
    "    return final_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b84910da-c548-422b-855c-1ae20b3bca26",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_list = keywords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3af98b22-111a-437c-8893-b6dc9697062c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['location',\n",
       " 'linear',\n",
       " 'estimation',\n",
       " 'spatial',\n",
       " 'quantity',\n",
       " 'combination',\n",
       " 'proximity',\n",
       " 'values',\n",
       " 'unobserved',\n",
       " 'z',\n",
       " 'mean',\n",
       " 'inference',\n",
       " 'weights',\n",
       " 'observed']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24405edd-5693-4c3d-b711-970a1c16b582",
   "metadata": {},
   "source": [
    "## Synonym Agent\n",
    "\n",
    "Traditional NLP methods proved unsuccessful in detecting synonyms in the dataset. This is primarily because NLP libraries struggle to identify multi-word terms as one term.\n",
    "\n",
    "The best solution proved to be using an open-source LLM with few-shot prompts as in-context learning to identify the alternate labels for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "057c1e8c-f54c-460b-a62c-edaa38d7928d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wiki_path</th>\n",
       "      <th>title</th>\n",
       "      <th>h2</th>\n",
       "      <th>h3</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cluster_analysis</td>\n",
       "      <td>cluster analysis</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cluster analysis or clustering is the task of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cluster_analysis</td>\n",
       "      <td>cluster analysis</td>\n",
       "      <td>Definition</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the notion of a \"cluster\" cannot be precisely ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cluster_analysis</td>\n",
       "      <td>cluster analysis</td>\n",
       "      <td>Algorithms</td>\n",
       "      <td>Connectivity-based clustering (hierarchical cl...</td>\n",
       "      <td>connectivity-based clustering, also known as h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cluster_analysis</td>\n",
       "      <td>cluster analysis</td>\n",
       "      <td>Algorithms</td>\n",
       "      <td>Centroid-based clustering</td>\n",
       "      <td>in centroid-based clustering, each cluster is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cluster_analysis</td>\n",
       "      <td>cluster analysis</td>\n",
       "      <td>Algorithms</td>\n",
       "      <td>Model-based clustering</td>\n",
       "      <td>the clustering framework most closely related ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          wiki_path             title            h2  \\\n",
       "0  cluster_analysis  cluster analysis  Introduction   \n",
       "1  cluster_analysis  cluster analysis    Definition   \n",
       "2  cluster_analysis  cluster analysis    Algorithms   \n",
       "3  cluster_analysis  cluster analysis    Algorithms   \n",
       "4  cluster_analysis  cluster analysis    Algorithms   \n",
       "\n",
       "                                                  h3  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2  Connectivity-based clustering (hierarchical cl...   \n",
       "3                          Centroid-based clustering   \n",
       "4                             Model-based clustering   \n",
       "\n",
       "                                             content  \n",
       "0  cluster analysis or clustering is the task of ...  \n",
       "1  the notion of a \"cluster\" cannot be precisely ...  \n",
       "2  connectivity-based clustering, also known as h...  \n",
       "3  in centroid-based clustering, each cluster is ...  \n",
       "4  the clustering framework most closely related ...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_content_latest = '../plagiarism_source.csv'\n",
    "wiki_content = pd.read_csv(wiki_content_latest)\n",
    "wiki_content.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e00bcca-32a7-452b-a85c-059cb6c11e02",
   "metadata": {},
   "source": [
    "## Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f87314d-7e6a-460b-b23e-7731a9f4ba2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "intro_rows = wiki_content[wiki_content['h2'] == 'Introduction']\n",
    "\n",
    "unique_titles = intro_rows['title'].unique()\n",
    "\n",
    "num_titles = min(3, len(unique_titles))\n",
    "random_titles = np.random.choice(unique_titles, size=num_titles, replace=False)\n",
    "\n",
    "result = intro_rows[intro_rows['title'].isin(random_titles)].groupby('title').first().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ae050958-75fb-4168-b155-f516a4762bd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    fuzzy clustering (also referred to as soft clu...\n",
       "1    a generative adversarial network ( gan ) is a ...\n",
       "2    synthetic data is information that is artifici...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "86541cf8-9f40-4e0d-afe4-bef8ec841d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_synonym_examples = [\n",
    "        {\n",
    "            \"title\": \"Linear Discriminant Analysis\",\n",
    "            \"text\": \"Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics and other fields, to find a linear combination of features that characterizes or separates two or more classes of objects or events.\",\n",
    "            \"answer\": [\"Linear Discriminant Analysis\", \"LDA\", \"Normal Discriminant Analysis\", \"NDA\", \"Discriminant Function Analysis\"]\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"Canonical correlation\",\n",
    "            \"text\": \"In statistics, canonical-correlation analysis (CCA), also called canonical variates analysis, is a way of...\",\n",
    "            \"answer\": [\"CCA\", \"Canonical variates analysis\"]\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"Non-negative matrix factorization\",\n",
    "            \"text\": \"Non-negative matrix factorization (NMF or NNMF), also non-negative matrix approximation[1][2] is a group of algorithms in multivariate analysis and linear algebra where...\",\n",
    "            \"answer\": [\"Non-negative matrix factorization\", \"NMF\", \"NNMF\"]\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"Support vector machine\",\n",
    "            \"text\": \"In machine learning, support vector machines (SVMs, also support vector networks[1]) is a method...\",\n",
    "            \"answer\": [\"Support vector machine\", \"SVM\", \"Support vector networks\"]\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0701664b-e9ef-4470-9ff9-c1d62a749842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_synonyms_initialisms(df):\n",
    "    \"\"\"\n",
    "    Extract synonyms and initialisms for the title from the content of a DataFrame row.\n",
    "\n",
    "    Uses GPT-3.5-turbo to identify synonyms and initialisms, then categorizes them.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): A DataFrame row containing 'title' and 'content' columns.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The input DataFrame row with added 'synonyms' and 'acronyms' columns.\n",
    "    \"\"\"\n",
    "    title = df['title']\n",
    "    text = df['content']\n",
    "    \n",
    "    few_shot_prompt = \"Here are some examples of identifying synonyms and initialisms:\\n\\n\"\n",
    "    for example in few_shot_synonym_examples:\n",
    "        few_shot_prompt += f\"Title: {example['title']}\\n\"\n",
    "        few_shot_prompt += f\"Text: {example['text']}\\n\"\n",
    "        few_shot_prompt += f\"Answer: {example['answer']}\\n\\n\"\n",
    "\n",
    "    full_prompt = (\n",
    "        f\"{few_shot_prompt}\"\n",
    "        f\"Now, identify any synonyms or initialisms for '{title}' from the following text:\\n\"\n",
    "        f\"{text}\\n\"\n",
    "        \"Return the results as a Python list.\"\n",
    "    )\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You detect synonyms and initialisms from text and respond in Python lists.\"},\n",
    "            {\"role\": \"user\", \"content\": full_prompt}\n",
    "        ]\n",
    "    )\n",
    "    answer = completion.choices[0].message.content\n",
    "    \n",
    "    try:\n",
    "        parsed_answer = eval(answer)\n",
    "        if isinstance(parsed_answer, list):\n",
    "            synonyms = []\n",
    "            acronyms = []\n",
    "            for item in parsed_answer:\n",
    "                if item.lower() == title.lower():\n",
    "                    continue\n",
    "                elif item.isupper() or re.search(r'[A-Z]{3,}', item):\n",
    "                    acronyms.append(item)\n",
    "                else:\n",
    "                    synonyms.append(item)\n",
    "            \n",
    "            df['synonyms'] = synonyms if synonyms else None\n",
    "            df['acronyms'] = acronyms if acronyms else None\n",
    "        else:\n",
    "            df['synonyms'] = None\n",
    "            df['acronyms'] = None\n",
    "    except:\n",
    "        df['synonyms'] = None\n",
    "        df['acronyms'] = None\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "462718df-1256-426d-a504-ccdf13de8358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fuzzy clustering', 'Soft clustering', 'Soft k-means']\n",
      "['Generative Adversarial Network', 'GAN', 'Machine Learning Frameworks', 'Generative AI']\n",
      "['Artificial data', 'Artificially generated data', 'Generated data', 'Algorithmically generated data']\n"
     ]
    }
   ],
   "source": [
    "result_with_synonyms = result.apply(extract_synonyms_initialisms, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "894397b7-a520-485b-b840-b6598a5ec6d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>wiki_path</th>\n",
       "      <th>h2</th>\n",
       "      <th>h3</th>\n",
       "      <th>content</th>\n",
       "      <th>synonyms</th>\n",
       "      <th>acronyms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fuzzy clustering</td>\n",
       "      <td>fuzzy_clustering</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>None</td>\n",
       "      <td>fuzzy clustering (also referred to as soft clu...</td>\n",
       "      <td>[Soft clustering, Soft k-means]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>generative adversarial network</td>\n",
       "      <td>generative_adversarial_network</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>None</td>\n",
       "      <td>a generative adversarial network ( gan ) is a ...</td>\n",
       "      <td>[Machine Learning Frameworks, Generative AI]</td>\n",
       "      <td>[GAN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>synthetic data</td>\n",
       "      <td>synthetic_data</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>None</td>\n",
       "      <td>synthetic data is information that is artifici...</td>\n",
       "      <td>[Artificial data, Artificially generated data,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            title                       wiki_path  \\\n",
       "0                fuzzy clustering                fuzzy_clustering   \n",
       "1  generative adversarial network  generative_adversarial_network   \n",
       "2                  synthetic data                  synthetic_data   \n",
       "\n",
       "             h2    h3                                            content  \\\n",
       "0  Introduction  None  fuzzy clustering (also referred to as soft clu...   \n",
       "1  Introduction  None  a generative adversarial network ( gan ) is a ...   \n",
       "2  Introduction  None  synthetic data is information that is artifici...   \n",
       "\n",
       "                                            synonyms acronyms  \n",
       "0                    [Soft clustering, Soft k-means]     None  \n",
       "1       [Machine Learning Frameworks, Generative AI]    [GAN]  \n",
       "2  [Artificial data, Artificially generated data,...     None  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_with_synonyms"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
