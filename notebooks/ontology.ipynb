{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4a21f5bb-04d8-472f-9194-cffa94ef1e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdbffd9-8dbf-46d4-a212-38eee035513c",
   "metadata": {},
   "source": [
    "# Ontology\n",
    "Ontology consists of our Taxonomy, entity labels, synonyms, initialisms/acroynms, and regular expressions for matching.\n",
    "\n",
    "### Challenges in building an Ontology of Terms\n",
    "\n",
    "Global Issues\n",
    "\n",
    "An Ontology term can appear in an article multiple forms:\n",
    "1. Terms may be abbreviated i.e. acronyms and initialisms\n",
    "2. Terms may be known by other names i.e. synonyms\n",
    "3. Terms may be generalised in nature\n",
    "4. Terms can change within the article i.e. term becomes initialised, singular v. plural"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfca1025-7b2b-416c-af64-8f1f567011df",
   "metadata": {},
   "source": [
    "## Unique terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24850805-b307-4cad-aba0-ff4b179dc31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueterms_path = '../unique_terms.csv'\n",
    "uniqueterms_df = pd.read_csv(uniqueterms_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e14acb1-2735-44b5-90e0-d39d8900a198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>taxonomy_term</th>\n",
       "      <th>term_regex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Trainable Layers</td>\n",
       "      <td>trainable(\\s*|\\-)?layer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cluster Analysis</td>\n",
       "      <td>cluster(\\s*|\\-)?analysi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Temperature</td>\n",
       "      <td>temperature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>One-Sample</td>\n",
       "      <td>one(\\s*|\\-)?sample</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Resnet</td>\n",
       "      <td>resnet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      taxonomy_term               term_regex\n",
       "0  Trainable Layers  trainable(\\s*|\\-)?layer\n",
       "1  Cluster Analysis  cluster(\\s*|\\-)?analysi\n",
       "2       Temperature              temperature\n",
       "3        One-Sample       one(\\s*|\\-)?sample\n",
       "4            Resnet                   resnet"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniqueterms_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184f6454-3d4e-42b8-9a62-6495420b6aad",
   "metadata": {},
   "source": [
    "## Wikipages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb03fd9-1232-4e4b-9809-ab2ac816fd19",
   "metadata": {},
   "source": [
    "The following code was used to generate an initial list of wikipage paths and titles in order to start building the Term Entity Recognition Model. \n",
    "\n",
    "Key Taxonomy terms (e.g. Supervised Learning) were manually identified on wikipedia to their page (\"Supervised_learning\") in order to identify further links we could use to initialise our ontology. \n",
    "\n",
    "\"\"\"\n",
    "def get_wiki_page_links(page_name):\n",
    "    url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"parse\",\n",
    "        \"page\": page_name,\n",
    "        \"prop\": \"links\",\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "    links_data = []\n",
    "    for link in data['parse']['links']:\n",
    "        if link['ns'] == 0:  # Only include main namespace links\n",
    "            title = link['*']\n",
    "            page_name = title.replace(' ', '_')\n",
    "            links_data.append({\n",
    "                'wiki_title': title,\n",
    "                'wiki_page': page_name,\n",
    "                'ignore': False\n",
    "            })\n",
    "\n",
    "    return links_data\n",
    "\n",
    "page_name = \"Supervised_learning\"\n",
    "page_name = \"Neural_network_(machine_learning)\"\n",
    "page_name = \"Outline_of_machine_learning\"\n",
    "page_name = \"Neural_network_(machine_learning)\"\n",
    "page_name = \"Machine_learning\"\n",
    "links = get_wiki_page_links(page_name)\n",
    "wikipages = pd.DataFrame(links)\n",
    "df.to_csv('wikipages.csv', index=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "23857ae3-2dec-403d-aa49-770dfd39bbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipages_path = '../wikipages_final.csv'\n",
    "wikipages_path_df = pd.read_csv(wikipages_path)\n",
    "\n",
    "wikipages_path_df = wikipages_path_df.drop(['ignore_path', 'redirect'], axis=1, errors='ignore')\n",
    "\n",
    "wikipages_path_df = wikipages_path_df.dropna(subset=['title', 'path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fab2d8-77cc-4f48-9e38-4e5c81cdd638",
   "metadata": {},
   "source": [
    "## Unique Wiki titles and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "68a847b4-99ae-4cf9-8c26-7a822aacd4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_wikipaths():\n",
    "    \"\"\"\n",
    "    Generate a DataFrame of unique Wikipedia paths and titles.\n",
    "\n",
    "    This function processes the global wikipages*path_df:\n",
    "    1. Extracts unique combinations of paths and titles\n",
    "    2. Converts paths and titles to lowercase and strips whitespace\n",
    "    3. Removes duplicate path-title combinations\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame with columns 'path' and 'title', \n",
    "                  containing unique lowercase path-title combinations.\n",
    "    \"\"\"\n",
    "    unique_wikipaths = []\n",
    "    wikipaths = set()\n",
    "    \n",
    "    for _, row in wikipages*path_df.iterrows():\n",
    "        path = row['path'].lower().strip()\n",
    "        title = row['title'].lower().strip()\n",
    "        \n",
    "        path_title_tuple = (path, title)\n",
    "        \n",
    "        if path_title_tuple not in wikipaths:\n",
    "            wikipaths.add(path_title_tuple)\n",
    "            unique_wikipaths.append({\n",
    "                'path': path,\n",
    "                'title': title\n",
    "            })\n",
    "    \n",
    "    unique_wikipaths_df = pd.DataFrame(unique_wikipaths)\n",
    "    \n",
    "    return unique_wikipaths_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "43a25845-d7f2-49eb-aab1-c74ff76ed8be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>machine_learning_(journal)</td>\n",
       "      <td>machine learning (journal)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>statistical_learning_in_language_acquisition</td>\n",
       "      <td>statistical learning in language acquisition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>statistical_learning</td>\n",
       "      <td>statistical learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>timeline_of_machine_learning</td>\n",
       "      <td>timeline of machine learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data_compression</td>\n",
       "      <td>data compression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10074</th>\n",
       "      <td>extended_backus%e2%80%93naur_form</td>\n",
       "      <td>extended backusâ€“naur form</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10075</th>\n",
       "      <td>feature_(machine_learning)</td>\n",
       "      <td>feature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10076</th>\n",
       "      <td>squared_error_loss</td>\n",
       "      <td>squared error loss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10077</th>\n",
       "      <td>gerard_salton</td>\n",
       "      <td>gerard salton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10078</th>\n",
       "      <td>fuzzy_clustering)</td>\n",
       "      <td>fuzzy c-means</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10079 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               path  \\\n",
       "0                        machine_learning_(journal)   \n",
       "1      statistical_learning_in_language_acquisition   \n",
       "2                              statistical_learning   \n",
       "3                      timeline_of_machine_learning   \n",
       "4                                  data_compression   \n",
       "...                                             ...   \n",
       "10074             extended_backus%e2%80%93naur_form   \n",
       "10075                    feature_(machine_learning)   \n",
       "10076                            squared_error_loss   \n",
       "10077                                 gerard_salton   \n",
       "10078                             fuzzy_clustering)   \n",
       "\n",
       "                                              title  \n",
       "0                        machine learning (journal)  \n",
       "1      statistical learning in language acquisition  \n",
       "2                              statistical learning  \n",
       "3                      timeline of machine learning  \n",
       "4                                  data compression  \n",
       "...                                             ...  \n",
       "10074                     extended backusâ€“naur form  \n",
       "10075                                       feature  \n",
       "10076                            squared error loss  \n",
       "10077                                 gerard salton  \n",
       "10078                                 fuzzy c-means  \n",
       "\n",
       "[10079 rows x 2 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_wikipaths_df = unique_wikipaths()\n",
    "unique_wikipaths_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690dcfa8-c511-4319-bf8d-62abab1c31d2",
   "metadata": {},
   "source": [
    "# Ontology linking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "212582e9-f4b5-4ee0-8c9e-98ee5d314af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ontology_links = uniqueterms_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "7c42a481-a83d-42ce-9812-d78a6037928e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>taxonomy_term</th>\n",
       "      <th>term_regex</th>\n",
       "      <th>wiki_title</th>\n",
       "      <th>wiki_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Trainable Layers</td>\n",
       "      <td>trainable(\\s*|\\-)?layer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cluster Analysis</td>\n",
       "      <td>cluster(\\s*|\\-)?analysi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Temperature</td>\n",
       "      <td>temperature</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>One-Sample</td>\n",
       "      <td>one(\\s*|\\-)?sample</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Resnet</td>\n",
       "      <td>resnet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>Feature Engineering</td>\n",
       "      <td>feature(\\s*|\\-)?engineering</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>Embedding Layer</td>\n",
       "      <td>embedding(\\s*|\\-)?layer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>Stacked Generalization</td>\n",
       "      <td>stacked(\\s*|\\-)?generalization</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>Underfitting</td>\n",
       "      <td>underfitting</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>Transposing</td>\n",
       "      <td>transposing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>537 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              taxonomy_term                      term_regex  wiki_title  \\\n",
       "0          Trainable Layers         trainable(\\s*|\\-)?layer         NaN   \n",
       "1          Cluster Analysis         cluster(\\s*|\\-)?analysi         NaN   \n",
       "2               Temperature                     temperature         NaN   \n",
       "3                One-Sample              one(\\s*|\\-)?sample         NaN   \n",
       "4                    Resnet                          resnet         NaN   \n",
       "..                      ...                             ...         ...   \n",
       "532     Feature Engineering     feature(\\s*|\\-)?engineering         NaN   \n",
       "533         Embedding Layer         embedding(\\s*|\\-)?layer         NaN   \n",
       "534  Stacked Generalization  stacked(\\s*|\\-)?generalization         NaN   \n",
       "535            Underfitting                    underfitting         NaN   \n",
       "536             Transposing                     transposing         NaN   \n",
       "\n",
       "     wiki_path  \n",
       "0          NaN  \n",
       "1          NaN  \n",
       "2          NaN  \n",
       "3          NaN  \n",
       "4          NaN  \n",
       "..         ...  \n",
       "532        NaN  \n",
       "533        NaN  \n",
       "534        NaN  \n",
       "535        NaN  \n",
       "536        NaN  \n",
       "\n",
       "[537 rows x 4 columns]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ontology_links['wiki_title'] = np.nan\n",
    "ontology_links['wiki_path'] = np.nan\n",
    "ontology_links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7470c906-be0f-4989-ac22-be94df144887",
   "metadata": {},
   "source": [
    "## Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "d0dd9d76-e432-432e-b377-68460d719e35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tax_term</th>\n",
       "      <th>title</th>\n",
       "      <th>path</th>\n",
       "      <th>wiki_path</th>\n",
       "      <th>wiki_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Least Absolute Shrinkage and Selection Operato...</td>\n",
       "      <td>Lasso (statistics)</td>\n",
       "      <td>Lasso (statistics)</td>\n",
       "      <td>lasso_(statistics)</td>\n",
       "      <td>lasso (statistics)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Temperature</td>\n",
       "      <td>Temperature (softmax function)</td>\n",
       "      <td>Temperature (softmax_function)</td>\n",
       "      <td>temperature_(softmax_function)</td>\n",
       "      <td>temperature (softmax function)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Meta-learning</td>\n",
       "      <td>Meta-learning (computer science)</td>\n",
       "      <td>Meta-learning (computer science)</td>\n",
       "      <td>meta-learning_(computer_science)</td>\n",
       "      <td>meta-learning (computer science)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>t-distributed stochastic neighbor embedding (t...</td>\n",
       "      <td>t-distributed stochastic neighbor embedding</td>\n",
       "      <td>t-distributed stochastic neighbor embedding</td>\n",
       "      <td>t-distributed_stochastic_neighbor_embedding</td>\n",
       "      <td>t-distributed stochastic neighbor embedding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>Gradient Descent Algorithms</td>\n",
       "      <td>Gradient Descent</td>\n",
       "      <td>Gradient Descent</td>\n",
       "      <td>gradient_descent</td>\n",
       "      <td>gradient descent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>78</td>\n",
       "      <td>Self-Attention Layer</td>\n",
       "      <td>Attention (machine learning)</td>\n",
       "      <td>Attention_(machine_learning)</td>\n",
       "      <td>attention_(machine_learning)</td>\n",
       "      <td>attention (machine learning)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>79</td>\n",
       "      <td>Shot Boundary Detection</td>\n",
       "      <td>Shot transition detection</td>\n",
       "      <td>Shot_transition_detection</td>\n",
       "      <td>shot_transition_detection</td>\n",
       "      <td>shot transition detection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>80</td>\n",
       "      <td>Stemming</td>\n",
       "      <td>Stemming</td>\n",
       "      <td>Stemming</td>\n",
       "      <td>stemming</td>\n",
       "      <td>stemming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>81</td>\n",
       "      <td>Variational Lower Bound</td>\n",
       "      <td>Evidence lower bound</td>\n",
       "      <td>Evidence_lower_bound</td>\n",
       "      <td>evidence_lower_bound</td>\n",
       "      <td>evidence lower bound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>86</td>\n",
       "      <td>Information Retrieval Metrics</td>\n",
       "      <td>Evaluation measures (information retrieval)</td>\n",
       "      <td>Evaluation_measures_(information_retrieval)</td>\n",
       "      <td>evaluation_measures_(information_retrieval)</td>\n",
       "      <td>evaluation measures (information retrieval)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                           tax_term  \\\n",
       "0    1  Least Absolute Shrinkage and Selection Operato...   \n",
       "1    2                                        Temperature   \n",
       "2    3                                      Meta-learning   \n",
       "3    5  t-distributed stochastic neighbor embedding (t...   \n",
       "4    6                        Gradient Descent Algorithms   \n",
       "..  ..                                                ...   \n",
       "73  78                               Self-Attention Layer   \n",
       "74  79                            Shot Boundary Detection   \n",
       "75  80                                           Stemming   \n",
       "76  81                            Variational Lower Bound   \n",
       "77  86                      Information Retrieval Metrics   \n",
       "\n",
       "                                          title  \\\n",
       "0                            Lasso (statistics)   \n",
       "1                Temperature (softmax function)   \n",
       "2              Meta-learning (computer science)   \n",
       "3   t-distributed stochastic neighbor embedding   \n",
       "4                              Gradient Descent   \n",
       "..                                          ...   \n",
       "73                 Attention (machine learning)   \n",
       "74                    Shot transition detection   \n",
       "75                                     Stemming   \n",
       "76                         Evidence lower bound   \n",
       "77  Evaluation measures (information retrieval)   \n",
       "\n",
       "                                           path  \\\n",
       "0                            Lasso (statistics)   \n",
       "1                Temperature (softmax_function)   \n",
       "2              Meta-learning (computer science)   \n",
       "3   t-distributed stochastic neighbor embedding   \n",
       "4                              Gradient Descent   \n",
       "..                                          ...   \n",
       "73                 Attention_(machine_learning)   \n",
       "74                    Shot_transition_detection   \n",
       "75                                     Stemming   \n",
       "76                         Evidence_lower_bound   \n",
       "77  Evaluation_measures_(information_retrieval)   \n",
       "\n",
       "                                      wiki_path  \\\n",
       "0                            lasso_(statistics)   \n",
       "1                temperature_(softmax_function)   \n",
       "2              meta-learning_(computer_science)   \n",
       "3   t-distributed_stochastic_neighbor_embedding   \n",
       "4                              gradient_descent   \n",
       "..                                          ...   \n",
       "73                 attention_(machine_learning)   \n",
       "74                    shot_transition_detection   \n",
       "75                                     stemming   \n",
       "76                         evidence_lower_bound   \n",
       "77  evaluation_measures_(information_retrieval)   \n",
       "\n",
       "                                     wiki_title  \n",
       "0                            lasso (statistics)  \n",
       "1                temperature (softmax function)  \n",
       "2              meta-learning (computer science)  \n",
       "3   t-distributed stochastic neighbor embedding  \n",
       "4                              gradient descent  \n",
       "..                                          ...  \n",
       "73                 attention (machine learning)  \n",
       "74                    shot transition detection  \n",
       "75                                     stemming  \n",
       "76                         evidence lower bound  \n",
       "77  evaluation measures (information retrieval)  \n",
       "\n",
       "[78 rows x 6 columns]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Add manual matches for strategically important terms\n",
    "path = '../../siads_capstone/db/wiki_data/wiki_search_man.csv' \n",
    "wikipages_manual_df = pd.read_csv(path)\n",
    "wikipages_manual_df.head()\n",
    "\n",
    "wikipages_manual_df['wiki_path'] = wikipages_manual_df['path'].str.strip().str.replace(' ', '_').str.lower()\n",
    "wikipages_manual_df['wiki_title'] = wikipages_manual_df['title'].str.strip().str.replace('_', ' ').str.lower()\n",
    "wikipages_manual_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "229caa4c-cc2a-470a-8631-b2d94e92385d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_wikipage_match(df1, df2):\n",
    "    \"\"\"\n",
    "    Match taxonomy terms from df1 with Wikipedia titles and paths from df2.\n",
    "\n",
    "    This function:\n",
    "    1. Normalizes terms in both DataFrames (strips whitespace and converts to lowercase)\n",
    "    2. Finds exact matches between df1's 'taxonomy_term' and df2's 'tax_term'\n",
    "    3. For each term in df1, collects matching titles and paths from df2\n",
    "    4. Joins multiple matches with '|' separator\n",
    "\n",
    "    Args:\n",
    "    df1 (pd.DataFrame): DataFrame containing 'taxonomy_term' column\n",
    "    df2 (pd.DataFrame): DataFrame containing 'tax_term', 'wiki_title', and 'wiki_path' columns\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: df1 with additional 'wiki_title' and 'wiki_path' columns\n",
    "    \"\"\"\n",
    "    wikipages_titles = []\n",
    "    wikipages_paths = []\n",
    "    \n",
    "    df1_terms = df1['taxonomy_term'].str.strip().str.lower()\n",
    "    df2_terms = df2['tax_term'].str.strip().str.lower()\n",
    "    \n",
    "    for term in df1_terms:\n",
    "        matches = df2_terms == term\n",
    "        \n",
    "        matching_titles = df2.loc[matches, 'wiki_title'].fillna('').astype(str)\n",
    "        matching_paths = df2.loc[matches, 'wiki_path'].fillna('').astype(str)\n",
    "        \n",
    "        wikipages_titles.append('|'.join(matching_titles) if not matching_titles.empty else None)\n",
    "        wikipages_paths.append('|'.join(matching_paths) if not matching_paths.empty else None)\n",
    "    \n",
    "    df1['wiki_title'] = wikipages_titles\n",
    "    df1['wiki_path'] = wikipages_paths\n",
    "    \n",
    "    return df1\n",
    "\n",
    "ontology_links_manual = manual_wikipage_match(ontology_links, wikipages_manual_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "a942a865-24fb-41f4-b335-df6c7cf56f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ontology_links_manual_matches = ontology_links_manual.dropna(subset=['wiki_title', 'wiki_path'])\n",
    "\n",
    "ontology_links_manual_unmatches = ontology_links_manual[ontology_links_manual['wiki_title'].isna() & ontology_links_manual['wiki_path'].isna()]\n",
    "\n",
    "ontology_links_manual_matches = ontology_links_manual_matches.reset_index(drop=True)\n",
    "ontology_links_manual_unmatches = ontology_links_manual_unmatches.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "20b8838b-6ba7-4658-88d1-e11c543e2e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matches(df1, df2):\n",
    "    \"\"\"\n",
    "    Match regex patterns from df1 with Wikipedia titles and paths from df2.\n",
    "\n",
    "    This function:\n",
    "    1. Normalizes terms in both DataFrames (strips whitespace and converts to lowercase)\n",
    "    2. Finds regex matches between df1's 'term_regex' and df2's 'title'\n",
    "    3. For multiple matches, selects the shortest matching title\n",
    "    4. For each term in df1, collects matching titles and paths from df2\n",
    "    5. Joins multiple matches with '|' separator\n",
    "\n",
    "    Args:\n",
    "    df1 (pd.DataFrame): DataFrame containing 'term_regex' column\n",
    "    df2 (pd.DataFrame): DataFrame containing 'title' and 'path' columns\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Copy of df1 with additional 'wiki_title' and 'wiki_path' columns\n",
    "    \"\"\"\n",
    "    df1 = df1.copy()\n",
    "    \n",
    "    wikipages_titles = []\n",
    "    wikipages_paths = []\n",
    "    \n",
    "    df1_terms = df1['term_regex'].str.strip().str.lower()\n",
    "    df2_terms = df2['title'].str.strip().str.lower()\n",
    "    \n",
    "    for term in df1_terms:\n",
    "        matches = df2_terms.str.match(f\"^{term}\", case=False)\n",
    "        \n",
    "        if matches.sum() > 1:\n",
    "            longest_match = df2_terms[matches].str.len().idxmin()\n",
    "            matches = matches & (df2_terms.index == longest_match)\n",
    "        \n",
    "        matching_titles = df2.loc[matches, 'title'].fillna('').astype(str)\n",
    "        matching_paths = df2.loc[matches, 'path'].fillna('').astype(str)\n",
    "        \n",
    "        wikipages_titles.append('|'.join(matching_titles) if not matching_titles.empty else None)\n",
    "        wikipages_paths.append('|'.join(matching_paths) if not matching_paths.empty else None)\n",
    "    \n",
    "    df1['wiki_title'] = wikipages_titles\n",
    "    df1['wiki_path'] = wikipages_paths\n",
    "    \n",
    "    return df1\n",
    "\n",
    "ontology_links = find_matches(ontology_links_manual_unmatches, unique_wikipaths_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "f74f6b3e-3011-44d6-b0c7-168f8d54d7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 462 entries, 0 to 461\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   taxonomy_term  462 non-null    object\n",
      " 1   term_regex     462 non-null    object\n",
      " 2   wiki_title     210 non-null    object\n",
      " 3   wiki_path      210 non-null    object\n",
      "dtypes: object(4)\n",
      "memory usage: 14.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(ontology_links.info())\n",
    "\n",
    "ontology_links_all = [ontology_links, ontology_links_manual_matches]\n",
    "\n",
    "ontology_wikilinks = pd.concat(ontology_links_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "c1b38d31-c563-4e2b-9e79-28b31121302a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matches: 285\n",
      "Percentage of matches: 53.07%\n",
      "Number of non-matches: 252\n",
      "\n",
      "Example matches:\n",
      "Taxonomy Term: Bhattacharyya Distance\n",
      "Term Regex: bhattacharyya(\\s*|\\-)?distance\n",
      "Matched Title: bhattacharyya distance\n",
      "Matched Path: bhattacharyya_distance\n",
      "--------------------------------------------------\n",
      "Taxonomy Term: Group Lasso\n",
      "Term Regex: group(\\s*|\\-)?lasso\n",
      "Matched Title: \n",
      "Matched Path: \n",
      "--------------------------------------------------\n",
      "Taxonomy Term: Temporal Difference Learning\n",
      "Term Regex: temporal(\\s*|\\-)?difference(\\s*|\\-)?learning\n",
      "Matched Title: temporal difference learning\n",
      "Matched Path: temporal_difference_learning\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "match_count = ontology_wikilinks['wiki_title'].notna().sum()\n",
    "total_rows = len(ontology_wikilinks)\n",
    "match_percentage = (match_count / total_rows) * 100\n",
    "\n",
    "print(f\"Number of matches: {match_count}\")\n",
    "print(f\"Percentage of matches: {match_percentage:.2f}%\")\n",
    "print(f\"Number of non-matches: {total_rows - match_count}\")\n",
    "\n",
    "print(\"\\nExample matches:\")\n",
    "match_examples = ontology_wikilinks[ontology_wikilinks['wiki_title'].notna()].sample(3)\n",
    "for _, row in match_examples.iterrows():\n",
    "    print(f\"Taxonomy Term: {row['taxonomy_term']}\")\n",
    "    print(f\"Term Regex: {row['term_regex']}\")\n",
    "    print(f\"Matched Title: {row['wiki_title']}\")\n",
    "    print(f\"Matched Path: {row['wiki_path']}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "d6bb8ece-860a-4771-a997-7daf18d17702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>taxonomy_term</th>\n",
       "      <th>term_regex</th>\n",
       "      <th>wiki_title</th>\n",
       "      <th>wiki_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Trainable Layers</td>\n",
       "      <td>trainable(\\s*|\\-)?layer</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cluster Analysis</td>\n",
       "      <td>cluster(\\s*|\\-)?analysi</td>\n",
       "      <td>cluster analysis</td>\n",
       "      <td>cluster_analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One-Sample</td>\n",
       "      <td>one(\\s*|\\-)?sample</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adapter Layers</td>\n",
       "      <td>adapter(\\s*|\\-)?layer</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sigmoid</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>sigmoid_function</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      taxonomy_term               term_regex        wiki_title  \\\n",
       "0  Trainable Layers  trainable(\\s*|\\-)?layer              None   \n",
       "1  Cluster Analysis  cluster(\\s*|\\-)?analysi  cluster analysis   \n",
       "2        One-Sample       one(\\s*|\\-)?sample              None   \n",
       "3    Adapter Layers    adapter(\\s*|\\-)?layer              None   \n",
       "4           Sigmoid                  sigmoid           sigmoid   \n",
       "\n",
       "          wiki_path  \n",
       "0              None  \n",
       "1  cluster_analysis  \n",
       "2              None  \n",
       "3              None  \n",
       "4  sigmoid_function  "
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ontology_wikilinks.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "184f3b04-c47a-4154-bf35-dfd0ee36e3d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>taxonomy_term</th>\n",
       "      <th>term_regex</th>\n",
       "      <th>wiki_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Feature Transformation</td>\n",
       "      <td>feature(\\s*|\\-)?transformation</td>\n",
       "      <td>data transformation (computing)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Data Transformation</td>\n",
       "      <td>data(\\s*|\\-)?transformation</td>\n",
       "      <td>data transformation (computing)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>Gradient Descent</td>\n",
       "      <td>gradient(\\s*|\\-)?descent</td>\n",
       "      <td>gradient descent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Gradient Descent Algorithms</td>\n",
       "      <td>gradient(\\s*|\\-)?descent(\\s*|\\-)?algorithm</td>\n",
       "      <td>gradient descent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Least-Angle Regression</td>\n",
       "      <td>least(\\s*|\\-)?angle(\\s*|\\-)?regression</td>\n",
       "      <td>least-angle regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Least Angle Regression</td>\n",
       "      <td>least(\\s*|\\-)?angle(\\s*|\\-)?regression</td>\n",
       "      <td>least-angle regression</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   taxonomy_term                                  term_regex  \\\n",
       "19        Feature Transformation              feature(\\s*|\\-)?transformation   \n",
       "63           Data Transformation                 data(\\s*|\\-)?transformation   \n",
       "418             Gradient Descent                    gradient(\\s*|\\-)?descent   \n",
       "16   Gradient Descent Algorithms  gradient(\\s*|\\-)?descent(\\s*|\\-)?algorithm   \n",
       "6         Least-Angle Regression      least(\\s*|\\-)?angle(\\s*|\\-)?regression   \n",
       "15        Least Angle Regression      least(\\s*|\\-)?angle(\\s*|\\-)?regression   \n",
       "\n",
       "                          wiki_title  \n",
       "19   data transformation (computing)  \n",
       "63   data transformation (computing)  \n",
       "418                 gradient descent  \n",
       "16                  gradient descent  \n",
       "6             least-angle regression  \n",
       "15            least-angle regression  "
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicates = ontology_wikilinks[ontology_wikilinks['wiki_title'].notna() & ontology_wikilinks['wiki_title'].duplicated(keep=False)]\n",
    "\n",
    "duplicates = duplicates.sort_values('wiki_title')\n",
    "\n",
    "# Display the duplicates\n",
    "duplicates[['taxonomy_term', 'term_regex', 'wiki_title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "3d1ee9e4-74fe-4ea2-a75d-a40fb27919ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>taxonomy_term</th>\n",
       "      <th>term_regex</th>\n",
       "      <th>wiki_title</th>\n",
       "      <th>wiki_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Trainable Layers</td>\n",
       "      <td>trainable(\\s*|\\-)?layer</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cluster Analysis</td>\n",
       "      <td>cluster(\\s*|\\-)?analysi</td>\n",
       "      <td>cluster analysis</td>\n",
       "      <td>cluster_analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One-Sample</td>\n",
       "      <td>one(\\s*|\\-)?sample</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adapter Layers</td>\n",
       "      <td>adapter(\\s*|\\-)?layer</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sigmoid</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>sigmoid_function</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      taxonomy_term               term_regex        wiki_title  \\\n",
       "0  Trainable Layers  trainable(\\s*|\\-)?layer              None   \n",
       "1  Cluster Analysis  cluster(\\s*|\\-)?analysi  cluster analysis   \n",
       "2        One-Sample       one(\\s*|\\-)?sample              None   \n",
       "3    Adapter Layers    adapter(\\s*|\\-)?layer              None   \n",
       "4           Sigmoid                  sigmoid           sigmoid   \n",
       "\n",
       "          wiki_path  \n",
       "0              None  \n",
       "1  cluster_analysis  \n",
       "2              None  \n",
       "3              None  \n",
       "4  sigmoid_function  "
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ontology_wikilinks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30eff4f-914d-4501-817c-f5c47f6e81e9",
   "metadata": {},
   "source": [
    "## Save files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "a6187078-003e-4ae4-99c2-26b4d2cf30f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique terms has been linked to wikipedia pages, saved as 'ontology_links.csv' in the following directory:\n",
      "/home/sagemaker-user/siads_capstone\n"
     ]
    }
   ],
   "source": [
    "relative_path = '../../siads_capstone/'\n",
    "    \n",
    "absolute_path = os.path.abspath(relative_path)\n",
    "    \n",
    "ontology_links_file_name = \"ontology_links.csv\"\n",
    "    \n",
    "ontology_links_file_path = os.path.join(absolute_path, ontology_links_file_name)\n",
    "\n",
    "ontology_wikilinks.to_csv(ontology_links_file_path, index=False)\n",
    "\n",
    "print(f\"unique terms has been linked to wikipedia pages, saved as '{ontology_links_file_name}' in the following directory:\")\n",
    "print(absolute_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
